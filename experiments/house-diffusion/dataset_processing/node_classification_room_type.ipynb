{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingyang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from utils import load_pickle\n",
    "\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting room type\n",
    "\n",
    "This notebook can be used to train a GNN to predict room type from zoning type. Then save graph_pred files for the test set that contain the predicted room_types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zoning_attribute(graph: nx.Graph):\n",
    "    room_type = nx.get_node_attributes(graph, 'room_type')\n",
    "\n",
    "    room_names = {node: constants.ROOM_NAMES[value] for node, value in room_type.items()}\n",
    "\n",
    "    inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "    room_names = {node: inv_room_mapping[value] for node, value in room_names.items()}\n",
    "\n",
    "    zoning = {key: constants.ZONING_MAPPING[value] for key, value in room_names.items()}\n",
    "\n",
    "    zoning_index = {key: constants.ZONING_NAMES.index(value) for key, value in zoning.items()}\n",
    "\n",
    "    nx.set_node_attributes(graph, zoning_index, 'zoning_type')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_hot_encode(value, num_classes):\n",
    "    return torch.eye(num_classes)[value]\n",
    "\n",
    "\n",
    "NUM_ROOM_TYPES = 9\n",
    "NUM_ZONING_TYPES = 4\n",
    "\n",
    "# def one_hot_encode_types(graph: nx.Graph):\n",
    "#     for node in graph.nodes:\n",
    "#         graph.nodes[node]['room_type'] = one_hot_encode(graph.nodes[node]['room_type'], NUM_ROOM_TYPES)\n",
    "#         graph.nodes[node]['zoning_type'] = one_hot_encode(graph.nodes[node]['zoning_type'], NUM_ZONING_TYPES)\n",
    "\n",
    "CONNECTIVITIES = [\"door\", \"entrance\", \"passage\"]\n",
    "\n",
    "class GraphZoningRoomTypeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, split=\"train\"):\n",
    "        # self.graph_in_path = os.path.join(path, 'graph_in')\n",
    "        self.graph_out_path = os.path.join(path, 'graph_out')\n",
    "\n",
    "        # include graph transformations if you like\n",
    "        # self.graph_transform = graph_transform\n",
    "\n",
    "        all_files = os.listdir(self.graph_out_path)\n",
    "\n",
    "        self.train_files, self.val_files = train_test_split(all_files, test_size=0.05, random_state=42)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.files = self.train_files\n",
    "        elif split == \"val\":\n",
    "            self.files = self.val_files\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_name = self.files[index]\n",
    "\n",
    "        # get access graph (name is index)\n",
    "        graph_nx = load_pickle(os.path.join(self.graph_out_path, file_name))\n",
    "\n",
    "        add_zoning_attribute(graph_nx)\n",
    "\n",
    "        for edge in graph_nx.edges:\n",
    "            graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "        # Remove attributes geometry, centroid\n",
    "        for node in graph_nx.nodes:\n",
    "            graph_nx.nodes[node].pop('geometry', None)\n",
    "            graph_nx.nodes[node].pop('centroid', None)\n",
    "\n",
    "        # graph_nx.graph[]\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"room_type\"] = one_hot_encode(graph_pyg[\"room_type\"], NUM_ROOM_TYPES)\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphZoningTypeTestSet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Graph Dataset. Collects NetworkX graph from a pre-defined folder and\n",
    "    transforms them to Pytorch Geometric (pyg.data.Data()) instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.graph_path = os.path.join(path, 'graph_in')\n",
    "\n",
    "        self.files = os.listdir(self.graph_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_name = self.files[index]\n",
    "\n",
    "        # get access graph (name is index)\n",
    "        graph_nx = load_pickle(os.path.join(self.graph_path, file_name))\n",
    "\n",
    "        for edge in graph_nx.edges:\n",
    "            graph_nx.edges[edge]['connectivity'] = CONNECTIVITIES.index(graph_nx.edges[edge]['connectivity'])\n",
    "\n",
    "        # transform networkx graph to pytorch geometric graph\n",
    "        graph_pyg = pyg.utils.from_networkx(graph_nx)\n",
    "\n",
    "        # transform graph if you like\n",
    "        graph_pyg = self.graph_transform(graph_pyg)\n",
    "\n",
    "        graph_pyg[\"file_name\"] = file_name\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_transform(graph_pyg):\n",
    "        graph_pyg[\"zoning_type\"] = one_hot_encode(graph_pyg[\"zoning_type\"], NUM_ZONING_TYPES)\n",
    "\n",
    "        graph_pyg[\"connectivity\"] = one_hot_encode(graph_pyg[\"connectivity\"], len(CONNECTIVITIES))\n",
    "\n",
    "        return graph_pyg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/path/to/modified-swiss-dwellings/modified-swiss-dwellings-v1-train/\"\n",
    "path = \"/home/mingyang/workspace/layout/dataset/modified-swiss-dwellings-v2/train\"\n",
    "\n",
    "# graph_nx = load_pickle(os.path.join(graph_path, f'{43}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_nx.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = GraphZoningRoomTypeDataset(path, split=\"train\")\n",
    "\n",
    "# A small portion of the training set is reserved for validation:\n",
    "ds_val = GraphZoningRoomTypeDataset(path, split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=NUM_ZONING_TYPES, num_edge_features=len(CONNECTIVITIES), hidden_size=32, target_size=NUM_ROOM_TYPES, num_hidden_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.convs = nn.ModuleList([GATConv(self.num_features if i == 0 else self.hidden_size, self.hidden_size, edge_dim=self.num_edge_features) for i in range(num_hidden_layers)])\n",
    "\n",
    "        self.hidden_linear = nn.Linear(self.hidden_size + num_features, self.hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.zoning_type, data.edge_index, data.connectivity\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # x = self.convs[-1](x, edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "        \n",
    "        x = torch.cat([x, data.zoning_type], dim=-1)\n",
    "\n",
    "        x = self.hidden_linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.relu(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_model(model, data_val, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracies = []\n",
    "    ious = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Batch size should be 1, because want to evaluate each graph separately\n",
    "        for data in DataLoader(data_val, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            pred = torch.argmax(model(data.to(device)), dim=-1)\n",
    "\n",
    "            gt = torch.argmax(data.room_type, dim=-1).to(device)\n",
    "            \n",
    "            # Tensor of shape (batch_size, 1)\n",
    "            acc = (gt == pred).sum(dim=-1) / pred.shape[-1]\n",
    "\n",
    "            pred_counter = Counter(pred.cpu().numpy())\n",
    "            gt_counter = Counter(gt.cpu().numpy())\n",
    "\n",
    "            iou = sum((pred_counter & gt_counter).values()) / sum((pred_counter | gt_counter).values())\n",
    "            ious.append(iou)\n",
    "\n",
    "            accuracies.append(acc.item())\n",
    "            \n",
    "            val_loss = F.cross_entropy(out, data.room_type)\n",
    "\n",
    "            total_loss += val_loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return total_loss, np.mean(accuracies), np.mean(ious)\n",
    "\n",
    "\n",
    "def train(model, data_train, data_val, batch_size, learning_rate, n_epochs=1, device=\"cpu\", save_loss_interval=1, print_interval=1):\n",
    "\n",
    "    NUM_TRAIN = len(data_train)\n",
    "    NUM_VAL = len(data_val)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # test_data = data_test[0]\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for data in loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(data)\n",
    "            \n",
    "            loss = F.cross_entropy(out, data.room_type)\n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % save_loss_interval == 0:\n",
    "            val_loss, val_acc, val_iou = evaluate_model(model, data_val, device=\"cpu\")\n",
    "\n",
    "            val_loss /= NUM_VAL\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            train_loss = epoch_loss / NUM_TRAIN\n",
    "          \n",
    "            if epoch % print_interval == 0:\n",
    "                print(f\"Epoch: {epoch} Train loss: {train_loss:.3e} Val loss: {val_loss:.3e} Val acc: {val_acc:.3f} Val iou: {val_iou:.3f}\")\n",
    "          \n",
    "            yield {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_iou\": val_iou,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    best_model = (None, 1e5, None)\n",
    "\n",
    "    for loss in train(model, ds_train, ds_val, batch_size=batch_size, learning_rate=learning_rate, n_epochs=n_epochs, device=device):\n",
    "        losses.append(loss)\n",
    "\n",
    "        if loss[\"val_loss\"] < best_model[1]:\n",
    "            best_model = (copy.deepcopy(model), loss[\"val_loss\"], loss)\n",
    "\n",
    "            print(f\"New best model with val loss: {loss['val_loss']:.3e}\")\n",
    "        \n",
    "        if loss[\"epoch\"] - best_model[2][\"epoch\"] > tolerance:\n",
    "            print(f\"Stopping early at epoch {loss['epoch']}\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"best_model\": best_model[0],\n",
    "        \"best_model_val_loss\": best_model[1],\n",
    "        \"best_model_loss_dict\": best_model[2],\n",
    "        \"last_model\": model,\n",
    "    }\n",
    "\n",
    "# train_with_early_stopping(model, ds_train, ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run hyperparameter search\n",
    "\n",
    "The num_hidden_layers parameter seemed to have the largest effect on accuracy, and thus a search is performed for this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 2 hidden layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingyang/.conda/envs/clip/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 6.063e-02 Val loss: 1.274e+00 Val acc: 0.684 Val iou: 0.574\n",
      "New best model with val loss: 1.274e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GATModel(num_hidden_layers\u001b[38;5;241m=\u001b[39mnum_hidden_layers)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hidden layers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(result, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroom_type_classifier_early_stopping_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain_with_early_stopping\u001b[0;34m(model, ds_train, ds_val, batch_size, learning_rate, n_epochs, device, tolerance)\u001b[0m\n\u001b[1;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m best_model \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1e5\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m train(model, ds_train, ds_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m     10\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m best_model[\u001b[38;5;241m1\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[8], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_train, data_val, batch_size, learning_rate, n_epochs, device, save_loss_interval, print_interval)\u001b[0m\n\u001b[1;32m     74\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_loss_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     val_loss, val_acc, val_iou \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m NUM_VAL\n\u001b[1;32m     81\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_val, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 21\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(data\u001b[38;5;241m.\u001b[39mroom_type, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Tensor of shape (batch_size, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mGATModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mzoning_type, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mconnectivity\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# adding edge features here!\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:349\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    346\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size) \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_nodes\n\u001b[1;32m    347\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m remove_self_loops(\n\u001b[1;32m    348\u001b[0m         edge_index, edge_attr)\n\u001b[0;32m--> 349\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43madd_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/loop.py:470\u001b[0m, in \u001b[0;36madd_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    464\u001b[0m     loop_index: Tensor \u001b[38;5;241m=\u001b[39m EdgeIndex(\n\u001b[1;32m    465\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    466\u001b[0m         sparse_size\u001b[38;5;241m=\u001b[39m(N, N),\n\u001b[1;32m    467\u001b[0m         is_undirected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m     loop_index \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    472\u001b[0m full_edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sparse:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num_hidden_layers in [2, 3, 4, 8, 16]:\n",
    "    model = GATModel(num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "    print(f\"Training model with {num_hidden_layers} hidden layers\")\n",
    "\n",
    "    result = train_with_early_stopping(model, ds_train, ds_val, batch_size=32, learning_rate=0.001, n_epochs=100, device=\"cuda\", tolerance=5)\n",
    "\n",
    "    torch.save(result, f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37282422832396356\n",
      "0.2846126250239439\n",
      "0.356501809406749\n",
      "0.6752528022470432\n",
      "0.5925173719116694\n"
     ]
    }
   ],
   "source": [
    "val_loss_dicts = []\n",
    "\n",
    "for num_hidden_layers in [2, 3, 4, 8, 16]:\n",
    "    result_i = torch.load( f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\", map_location=\"cpu\")\n",
    "\n",
    "    print(result_i[\"best_model_val_loss\"])\n",
    "\n",
    "    val_loss_dicts.append({\n",
    "        \"num_hidden_layers\": num_hidden_layers,\n",
    "        \"val_loss\": result_i[\"best_model_val_loss\"],\n",
    "        # \"max_epoch\": result_i[\"losses\"][-1][\"epoch\"],\n",
    "        \"val_acc\": result_i[\"best_model_loss_dict\"][\"val_acc\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['losses', 'best_model', 'best_model_val_loss', 'best_model_loss_dict', 'last_model'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_i.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='num_hidden_layers'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(val_loss_dicts).plot.bar(x=\"num_hidden_layers\", y=\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "num hidden layers & val loss & val acc \\\\\n",
      "2 & 0.37 & 0.87 \\\\\n",
      "3 & 0.28 & 0.90 \\\\\n",
      "4 & 0.36 & 0.84 \\\\\n",
      "8 & 0.68 & 0.76 \\\\\n",
      "16 & 0.59 & 0.73 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(val_loss_dicts)[[\"num_hidden_layers\", \"val_loss\", \"val_acc\"]].style.hide(axis=\"index\").format(lambda x: f\"{x:.2f}\", [\"val_loss\", \"val_acc\"]).to_latex().replace(\"_\", \" \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one of the models from the hp search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which `num_hidden_layers` run to load:\n",
    "num_hidden_layers = 3\n",
    "results_dict = torch.load(f\"room_type_classifier_early_stopping_results_{num_hidden_layers}.pt\")\n",
    "\n",
    "model = results_dict[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(4, 32, heads=1)\n",
       "    (1-2): 2 x GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (hidden_linear): Linear(in_features=36, out_features=32, bias=True)\n",
       "  (linear): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 80], zoning_type=[38, 4], connectivity=[80, 3], num_nodes=38, file_name='4167.pickle')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = GraphZoningTypeTestSet('/path/to/modified-swiss-dwellings-v1-test/')\n",
    "\n",
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GraphZoningTypeTestSet at 0x7f2340c57190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make folder for output graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_graph_path = ds_test.graph_path.replace(\"graph_in\", \"graph_pred\")\n",
    "\n",
    "os.makedirs(pred_graph_path, exist_ok=True)\n",
    "\n",
    "pred_graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graph_pred graphs\n",
    "\n",
    "Run inference on the test graph_in files, and save the graph_pred graphs with the added room type attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167.pickle\n",
      "4168.pickle\n",
      "4169.pickle\n",
      "4170.pickle\n",
      "4171.pickle\n",
      "4172.pickle\n",
      "4173.pickle\n",
      "4174.pickle\n",
      "4175.pickle\n",
      "4176.pickle\n",
      "4177.pickle\n",
      "4178.pickle\n",
      "4179.pickle\n",
      "4180.pickle\n",
      "4181.pickle\n",
      "4182.pickle\n",
      "4183.pickle\n",
      "4184.pickle\n",
      "4185.pickle\n",
      "4186.pickle\n",
      "4187.pickle\n",
      "4188.pickle\n",
      "4189.pickle\n",
      "4190.pickle\n",
      "4191.pickle\n",
      "4192.pickle\n",
      "4193.pickle\n",
      "4194.pickle\n",
      "4195.pickle\n",
      "4196.pickle\n",
      "4197.pickle\n",
      "4198.pickle\n",
      "4199.pickle\n",
      "4200.pickle\n",
      "4201.pickle\n",
      "4202.pickle\n",
      "4203.pickle\n",
      "4204.pickle\n",
      "4205.pickle\n",
      "4206.pickle\n",
      "4207.pickle\n",
      "4208.pickle\n",
      "4209.pickle\n",
      "4210.pickle\n",
      "4211.pickle\n",
      "4212.pickle\n",
      "4213.pickle\n",
      "4214.pickle\n",
      "4215.pickle\n",
      "4216.pickle\n",
      "4217.pickle\n",
      "4218.pickle\n",
      "4219.pickle\n",
      "4220.pickle\n",
      "4221.pickle\n",
      "4222.pickle\n",
      "4223.pickle\n",
      "4224.pickle\n",
      "4225.pickle\n",
      "4226.pickle\n",
      "4227.pickle\n",
      "4228.pickle\n",
      "4229.pickle\n",
      "4230.pickle\n",
      "4231.pickle\n",
      "4232.pickle\n",
      "4233.pickle\n",
      "4234.pickle\n",
      "4235.pickle\n",
      "4236.pickle\n",
      "4237.pickle\n",
      "4238.pickle\n",
      "4239.pickle\n",
      "4240.pickle\n",
      "4241.pickle\n",
      "4242.pickle\n",
      "4243.pickle\n",
      "4244.pickle\n",
      "4245.pickle\n",
      "4246.pickle\n",
      "4247.pickle\n",
      "4248.pickle\n",
      "4249.pickle\n",
      "4250.pickle\n",
      "4251.pickle\n",
      "4252.pickle\n",
      "4253.pickle\n",
      "4254.pickle\n",
      "4255.pickle\n",
      "4256.pickle\n",
      "4257.pickle\n",
      "4258.pickle\n",
      "4259.pickle\n",
      "4260.pickle\n",
      "4261.pickle\n",
      "4262.pickle\n",
      "4263.pickle\n",
      "4264.pickle\n",
      "4265.pickle\n",
      "4266.pickle\n",
      "4267.pickle\n",
      "4268.pickle\n",
      "4269.pickle\n",
      "4270.pickle\n",
      "4271.pickle\n",
      "4272.pickle\n",
      "4273.pickle\n",
      "4274.pickle\n",
      "4275.pickle\n",
      "4276.pickle\n",
      "4277.pickle\n",
      "4278.pickle\n",
      "4279.pickle\n",
      "4280.pickle\n",
      "4281.pickle\n",
      "4282.pickle\n",
      "4283.pickle\n",
      "4284.pickle\n",
      "4285.pickle\n",
      "4286.pickle\n",
      "4287.pickle\n",
      "4288.pickle\n",
      "4289.pickle\n",
      "4290.pickle\n",
      "4291.pickle\n",
      "4292.pickle\n",
      "4293.pickle\n",
      "4294.pickle\n",
      "4295.pickle\n",
      "4296.pickle\n",
      "4297.pickle\n",
      "4298.pickle\n",
      "4299.pickle\n",
      "4300.pickle\n",
      "4301.pickle\n",
      "4302.pickle\n",
      "4303.pickle\n",
      "4304.pickle\n",
      "4305.pickle\n",
      "4306.pickle\n",
      "4307.pickle\n",
      "4308.pickle\n",
      "4309.pickle\n",
      "4310.pickle\n",
      "4311.pickle\n",
      "4312.pickle\n",
      "4313.pickle\n",
      "4314.pickle\n",
      "4315.pickle\n",
      "4316.pickle\n",
      "4317.pickle\n",
      "4318.pickle\n",
      "4319.pickle\n",
      "4320.pickle\n",
      "4321.pickle\n",
      "4322.pickle\n",
      "4323.pickle\n",
      "4324.pickle\n",
      "4325.pickle\n",
      "4326.pickle\n",
      "4327.pickle\n",
      "4328.pickle\n",
      "4329.pickle\n",
      "4330.pickle\n",
      "4331.pickle\n",
      "4332.pickle\n",
      "4333.pickle\n",
      "4334.pickle\n",
      "4335.pickle\n",
      "4336.pickle\n",
      "4337.pickle\n",
      "4338.pickle\n",
      "4339.pickle\n",
      "4340.pickle\n",
      "4341.pickle\n",
      "4342.pickle\n",
      "4343.pickle\n",
      "4344.pickle\n",
      "4345.pickle\n",
      "4346.pickle\n",
      "4347.pickle\n",
      "4348.pickle\n",
      "4349.pickle\n",
      "4350.pickle\n",
      "4351.pickle\n",
      "4352.pickle\n",
      "4353.pickle\n",
      "4354.pickle\n",
      "4355.pickle\n",
      "4356.pickle\n",
      "4357.pickle\n",
      "4358.pickle\n",
      "4359.pickle\n",
      "4360.pickle\n",
      "4361.pickle\n",
      "4362.pickle\n",
      "4363.pickle\n",
      "4364.pickle\n",
      "4365.pickle\n",
      "4366.pickle\n",
      "4367.pickle\n",
      "4368.pickle\n",
      "4369.pickle\n",
      "4370.pickle\n",
      "4371.pickle\n",
      "4372.pickle\n",
      "4373.pickle\n",
      "4374.pickle\n",
      "4375.pickle\n",
      "4376.pickle\n",
      "4377.pickle\n",
      "4378.pickle\n",
      "4379.pickle\n",
      "4380.pickle\n",
      "4381.pickle\n",
      "4382.pickle\n",
      "4383.pickle\n",
      "4384.pickle\n",
      "4385.pickle\n",
      "4386.pickle\n",
      "4387.pickle\n",
      "4388.pickle\n",
      "4389.pickle\n",
      "4390.pickle\n",
      "4391.pickle\n",
      "4392.pickle\n",
      "4393.pickle\n",
      "4394.pickle\n",
      "4395.pickle\n",
      "4396.pickle\n",
      "4397.pickle\n",
      "4398.pickle\n",
      "4399.pickle\n",
      "4400.pickle\n",
      "4401.pickle\n",
      "4402.pickle\n",
      "4403.pickle\n",
      "4404.pickle\n",
      "4405.pickle\n",
      "4406.pickle\n",
      "4407.pickle\n",
      "4408.pickle\n",
      "4409.pickle\n",
      "4410.pickle\n",
      "4411.pickle\n",
      "4412.pickle\n",
      "4413.pickle\n",
      "4414.pickle\n",
      "4415.pickle\n",
      "4416.pickle\n",
      "4417.pickle\n",
      "4418.pickle\n",
      "4419.pickle\n",
      "4420.pickle\n",
      "4421.pickle\n",
      "4422.pickle\n",
      "4423.pickle\n",
      "4424.pickle\n",
      "4425.pickle\n",
      "4426.pickle\n",
      "4427.pickle\n",
      "4428.pickle\n",
      "4429.pickle\n",
      "4430.pickle\n",
      "4431.pickle\n",
      "4432.pickle\n",
      "4433.pickle\n",
      "4434.pickle\n",
      "4435.pickle\n",
      "4436.pickle\n",
      "4437.pickle\n",
      "4438.pickle\n",
      "4439.pickle\n",
      "4440.pickle\n",
      "4441.pickle\n",
      "4442.pickle\n",
      "4443.pickle\n",
      "4444.pickle\n",
      "4445.pickle\n",
      "4446.pickle\n",
      "4447.pickle\n",
      "4448.pickle\n",
      "4449.pickle\n",
      "4450.pickle\n",
      "4451.pickle\n",
      "4452.pickle\n",
      "4453.pickle\n",
      "4454.pickle\n",
      "4455.pickle\n",
      "4456.pickle\n",
      "4457.pickle\n",
      "4458.pickle\n",
      "4459.pickle\n",
      "4460.pickle\n",
      "4461.pickle\n",
      "4462.pickle\n",
      "4463.pickle\n",
      "4464.pickle\n",
      "4465.pickle\n",
      "4466.pickle\n",
      "4467.pickle\n",
      "4468.pickle\n",
      "4469.pickle\n",
      "4470.pickle\n",
      "4471.pickle\n",
      "4472.pickle\n",
      "4473.pickle\n",
      "4474.pickle\n",
      "4475.pickle\n",
      "4476.pickle\n",
      "4477.pickle\n",
      "4478.pickle\n",
      "4479.pickle\n",
      "4480.pickle\n",
      "4481.pickle\n",
      "4482.pickle\n",
      "4483.pickle\n",
      "4484.pickle\n",
      "4485.pickle\n",
      "4486.pickle\n",
      "4487.pickle\n",
      "4488.pickle\n",
      "4489.pickle\n",
      "4490.pickle\n",
      "4491.pickle\n",
      "4492.pickle\n",
      "4493.pickle\n",
      "4494.pickle\n",
      "4495.pickle\n",
      "4496.pickle\n",
      "4497.pickle\n",
      "4498.pickle\n",
      "4499.pickle\n",
      "4500.pickle\n",
      "4501.pickle\n",
      "4502.pickle\n",
      "4503.pickle\n",
      "4504.pickle\n",
      "4505.pickle\n",
      "4506.pickle\n",
      "4507.pickle\n",
      "4508.pickle\n",
      "4509.pickle\n",
      "4510.pickle\n",
      "4511.pickle\n",
      "4512.pickle\n",
      "4513.pickle\n",
      "4514.pickle\n",
      "4515.pickle\n",
      "4516.pickle\n",
      "4517.pickle\n",
      "4518.pickle\n",
      "4519.pickle\n",
      "4520.pickle\n",
      "4521.pickle\n",
      "4522.pickle\n",
      "4523.pickle\n",
      "4524.pickle\n",
      "4525.pickle\n",
      "4526.pickle\n",
      "4527.pickle\n",
      "4528.pickle\n",
      "4529.pickle\n",
      "4530.pickle\n",
      "4531.pickle\n",
      "4532.pickle\n",
      "4533.pickle\n",
      "4534.pickle\n",
      "4535.pickle\n",
      "4536.pickle\n",
      "4537.pickle\n",
      "4538.pickle\n",
      "4539.pickle\n",
      "4540.pickle\n",
      "4541.pickle\n",
      "4542.pickle\n",
      "4543.pickle\n",
      "4544.pickle\n",
      "4545.pickle\n",
      "4546.pickle\n",
      "4547.pickle\n",
      "4548.pickle\n",
      "4549.pickle\n",
      "4550.pickle\n",
      "4551.pickle\n",
      "4552.pickle\n",
      "4553.pickle\n",
      "4554.pickle\n",
      "4555.pickle\n",
      "4556.pickle\n",
      "4557.pickle\n",
      "4558.pickle\n",
      "4559.pickle\n",
      "4560.pickle\n",
      "4561.pickle\n",
      "4562.pickle\n",
      "4563.pickle\n",
      "4564.pickle\n",
      "4565.pickle\n",
      "4566.pickle\n",
      "4567.pickle\n",
      "4568.pickle\n",
      "4569.pickle\n",
      "4570.pickle\n",
      "4571.pickle\n",
      "4572.pickle\n",
      "4573.pickle\n",
      "4574.pickle\n",
      "4575.pickle\n",
      "4576.pickle\n",
      "4577.pickle\n",
      "4578.pickle\n",
      "4579.pickle\n",
      "4580.pickle\n",
      "4581.pickle\n",
      "4582.pickle\n",
      "4583.pickle\n",
      "4584.pickle\n",
      "4585.pickle\n",
      "4586.pickle\n",
      "4587.pickle\n",
      "4588.pickle\n",
      "4589.pickle\n",
      "4590.pickle\n",
      "4591.pickle\n",
      "4592.pickle\n",
      "4593.pickle\n",
      "4594.pickle\n",
      "4595.pickle\n",
      "4596.pickle\n",
      "4597.pickle\n",
      "4598.pickle\n",
      "4599.pickle\n",
      "4600.pickle\n",
      "4601.pickle\n",
      "4602.pickle\n",
      "4603.pickle\n",
      "4604.pickle\n",
      "4605.pickle\n",
      "4606.pickle\n",
      "4607.pickle\n",
      "4608.pickle\n",
      "4609.pickle\n",
      "4610.pickle\n",
      "4611.pickle\n",
      "4612.pickle\n",
      "4613.pickle\n",
      "4614.pickle\n",
      "4615.pickle\n",
      "4616.pickle\n",
      "4617.pickle\n",
      "4618.pickle\n",
      "4619.pickle\n",
      "4620.pickle\n",
      "4621.pickle\n",
      "4622.pickle\n",
      "4623.pickle\n",
      "4624.pickle\n",
      "4625.pickle\n",
      "4626.pickle\n",
      "4627.pickle\n",
      "4628.pickle\n",
      "4629.pickle\n",
      "4630.pickle\n",
      "4631.pickle\n",
      "4632.pickle\n",
      "4633.pickle\n",
      "4634.pickle\n",
      "4635.pickle\n",
      "4636.pickle\n",
      "4637.pickle\n",
      "4638.pickle\n",
      "4639.pickle\n",
      "4640.pickle\n",
      "4641.pickle\n",
      "4642.pickle\n",
      "4643.pickle\n",
      "4644.pickle\n",
      "4645.pickle\n",
      "4646.pickle\n",
      "4647.pickle\n",
      "4648.pickle\n",
      "4649.pickle\n",
      "4650.pickle\n",
      "4651.pickle\n",
      "4652.pickle\n",
      "4653.pickle\n",
      "4654.pickle\n",
      "4655.pickle\n",
      "4656.pickle\n",
      "4657.pickle\n",
      "4658.pickle\n",
      "4659.pickle\n",
      "4660.pickle\n",
      "4661.pickle\n",
      "4662.pickle\n",
      "4663.pickle\n",
      "4664.pickle\n",
      "4665.pickle\n",
      "4666.pickle\n",
      "4667.pickle\n",
      "4668.pickle\n",
      "4669.pickle\n",
      "4670.pickle\n",
      "4671.pickle\n",
      "4672.pickle\n",
      "4673.pickle\n",
      "4674.pickle\n",
      "4675.pickle\n",
      "4676.pickle\n",
      "4677.pickle\n",
      "4678.pickle\n",
      "4679.pickle\n",
      "4680.pickle\n",
      "4681.pickle\n",
      "4682.pickle\n",
      "4683.pickle\n",
      "4684.pickle\n",
      "4685.pickle\n",
      "4686.pickle\n",
      "4687.pickle\n",
      "4688.pickle\n",
      "4689.pickle\n",
      "4690.pickle\n",
      "4691.pickle\n",
      "4692.pickle\n",
      "4693.pickle\n",
      "4694.pickle\n",
      "4695.pickle\n",
      "4696.pickle\n",
      "4697.pickle\n",
      "4698.pickle\n",
      "4699.pickle\n",
      "4700.pickle\n",
      "4701.pickle\n",
      "4702.pickle\n",
      "4703.pickle\n",
      "4704.pickle\n",
      "4705.pickle\n",
      "4706.pickle\n",
      "4707.pickle\n",
      "4708.pickle\n",
      "4709.pickle\n",
      "4710.pickle\n",
      "4711.pickle\n",
      "4712.pickle\n",
      "4713.pickle\n",
      "4714.pickle\n",
      "4715.pickle\n",
      "4716.pickle\n",
      "4717.pickle\n",
      "4718.pickle\n",
      "4719.pickle\n",
      "4720.pickle\n",
      "4721.pickle\n",
      "4722.pickle\n",
      "4723.pickle\n",
      "4724.pickle\n",
      "4725.pickle\n",
      "4726.pickle\n",
      "4727.pickle\n",
      "4728.pickle\n",
      "4729.pickle\n",
      "4730.pickle\n",
      "4731.pickle\n",
      "4732.pickle\n",
      "4733.pickle\n",
      "4734.pickle\n",
      "4735.pickle\n",
      "4736.pickle\n",
      "4737.pickle\n",
      "4738.pickle\n",
      "4739.pickle\n",
      "4740.pickle\n",
      "4741.pickle\n",
      "4742.pickle\n",
      "4743.pickle\n",
      "4744.pickle\n",
      "4745.pickle\n",
      "4746.pickle\n",
      "4747.pickle\n",
      "4748.pickle\n",
      "4749.pickle\n",
      "4750.pickle\n",
      "4751.pickle\n",
      "4752.pickle\n",
      "4753.pickle\n",
      "4754.pickle\n",
      "4755.pickle\n",
      "4756.pickle\n",
      "4757.pickle\n",
      "4758.pickle\n",
      "4759.pickle\n",
      "4760.pickle\n",
      "4761.pickle\n",
      "4762.pickle\n",
      "4763.pickle\n",
      "4764.pickle\n",
      "4765.pickle\n",
      "4766.pickle\n",
      "4767.pickle\n",
      "4768.pickle\n",
      "4769.pickle\n",
      "4770.pickle\n",
      "4771.pickle\n",
      "4772.pickle\n",
      "4773.pickle\n",
      "4774.pickle\n",
      "4775.pickle\n",
      "4776.pickle\n",
      "4777.pickle\n",
      "4778.pickle\n",
      "4779.pickle\n",
      "4780.pickle\n",
      "4781.pickle\n",
      "4782.pickle\n",
      "4783.pickle\n",
      "4784.pickle\n",
      "4785.pickle\n",
      "4786.pickle\n",
      "4787.pickle\n",
      "4788.pickle\n",
      "4789.pickle\n",
      "4790.pickle\n",
      "4791.pickle\n",
      "4792.pickle\n",
      "4793.pickle\n",
      "4794.pickle\n",
      "4795.pickle\n",
      "4796.pickle\n",
      "4797.pickle\n",
      "4798.pickle\n",
      "4799.pickle\n",
      "4800.pickle\n",
      "4801.pickle\n",
      "4802.pickle\n",
      "4803.pickle\n",
      "4804.pickle\n",
      "4805.pickle\n",
      "4806.pickle\n",
      "4807.pickle\n",
      "4808.pickle\n",
      "4809.pickle\n",
      "4810.pickle\n",
      "4811.pickle\n",
      "4812.pickle\n",
      "4813.pickle\n",
      "4814.pickle\n",
      "4815.pickle\n",
      "4816.pickle\n",
      "4817.pickle\n",
      "4818.pickle\n",
      "4819.pickle\n",
      "4820.pickle\n",
      "4821.pickle\n",
      "4822.pickle\n",
      "4823.pickle\n",
      "4824.pickle\n",
      "4825.pickle\n",
      "4826.pickle\n",
      "4827.pickle\n",
      "4828.pickle\n",
      "4829.pickle\n",
      "4830.pickle\n",
      "4831.pickle\n",
      "4832.pickle\n",
      "4833.pickle\n",
      "4834.pickle\n",
      "4835.pickle\n",
      "4836.pickle\n",
      "4837.pickle\n",
      "4838.pickle\n",
      "4839.pickle\n",
      "4840.pickle\n",
      "4841.pickle\n",
      "4842.pickle\n",
      "4843.pickle\n",
      "4844.pickle\n",
      "4845.pickle\n",
      "4846.pickle\n",
      "4847.pickle\n",
      "4848.pickle\n",
      "4849.pickle\n",
      "4850.pickle\n",
      "4851.pickle\n",
      "4852.pickle\n",
      "4853.pickle\n",
      "4854.pickle\n",
      "4855.pickle\n",
      "4856.pickle\n",
      "4857.pickle\n",
      "4858.pickle\n",
      "4859.pickle\n",
      "4860.pickle\n",
      "4861.pickle\n",
      "4862.pickle\n",
      "4863.pickle\n",
      "4864.pickle\n",
      "4865.pickle\n",
      "4866.pickle\n",
      "4867.pickle\n",
      "4868.pickle\n",
      "4869.pickle\n",
      "4870.pickle\n",
      "4871.pickle\n",
      "4872.pickle\n",
      "4873.pickle\n",
      "4874.pickle\n",
      "4875.pickle\n",
      "4876.pickle\n",
      "4877.pickle\n",
      "4878.pickle\n",
      "4879.pickle\n",
      "4880.pickle\n",
      "4881.pickle\n",
      "4882.pickle\n",
      "4883.pickle\n",
      "4884.pickle\n",
      "4885.pickle\n",
      "4886.pickle\n",
      "4887.pickle\n",
      "4888.pickle\n",
      "4889.pickle\n",
      "4890.pickle\n",
      "4891.pickle\n",
      "4892.pickle\n",
      "4893.pickle\n",
      "4894.pickle\n",
      "4895.pickle\n",
      "4896.pickle\n",
      "4897.pickle\n",
      "4898.pickle\n",
      "4899.pickle\n",
      "4900.pickle\n",
      "4901.pickle\n",
      "4902.pickle\n",
      "4903.pickle\n",
      "4904.pickle\n",
      "4905.pickle\n",
      "4906.pickle\n",
      "4907.pickle\n",
      "4908.pickle\n",
      "4909.pickle\n",
      "4910.pickle\n",
      "4911.pickle\n",
      "4912.pickle\n",
      "4913.pickle\n",
      "4914.pickle\n",
      "4915.pickle\n",
      "4916.pickle\n",
      "4917.pickle\n",
      "4918.pickle\n",
      "4919.pickle\n",
      "4920.pickle\n",
      "4921.pickle\n",
      "4922.pickle\n",
      "4923.pickle\n",
      "4924.pickle\n",
      "4925.pickle\n",
      "4926.pickle\n",
      "4927.pickle\n",
      "4928.pickle\n",
      "4929.pickle\n",
      "4930.pickle\n",
      "4931.pickle\n",
      "4932.pickle\n",
      "4933.pickle\n",
      "4934.pickle\n",
      "4935.pickle\n",
      "4936.pickle\n",
      "4937.pickle\n",
      "4938.pickle\n",
      "4939.pickle\n",
      "4940.pickle\n",
      "4941.pickle\n",
      "4942.pickle\n",
      "4943.pickle\n",
      "4944.pickle\n",
      "4945.pickle\n",
      "4946.pickle\n",
      "4947.pickle\n",
      "4948.pickle\n",
      "4949.pickle\n",
      "4950.pickle\n",
      "4951.pickle\n",
      "4952.pickle\n",
      "4953.pickle\n",
      "4954.pickle\n",
      "4955.pickle\n",
      "4956.pickle\n",
      "4957.pickle\n",
      "4958.pickle\n",
      "4959.pickle\n",
      "4960.pickle\n",
      "4961.pickle\n",
      "4962.pickle\n",
      "4963.pickle\n",
      "4964.pickle\n",
      "4965.pickle\n",
      "4966.pickle\n",
      "4967.pickle\n",
      "4968.pickle\n",
      "4969.pickle\n",
      "4970.pickle\n",
      "4971.pickle\n",
      "4972.pickle\n",
      "4973.pickle\n",
      "4974.pickle\n",
      "4975.pickle\n",
      "4976.pickle\n",
      "4977.pickle\n",
      "4978.pickle\n",
      "4979.pickle\n",
      "4980.pickle\n",
      "4981.pickle\n",
      "4982.pickle\n",
      "4983.pickle\n",
      "4984.pickle\n",
      "4985.pickle\n",
      "4986.pickle\n",
      "4987.pickle\n",
      "4988.pickle\n",
      "4989.pickle\n",
      "4990.pickle\n",
      "4991.pickle\n",
      "4992.pickle\n",
      "4993.pickle\n",
      "4994.pickle\n",
      "4995.pickle\n",
      "4996.pickle\n",
      "4997.pickle\n",
      "4998.pickle\n",
      "4999.pickle\n",
      "5000.pickle\n",
      "5001.pickle\n",
      "5002.pickle\n",
      "5003.pickle\n",
      "5004.pickle\n",
      "5005.pickle\n",
      "5006.pickle\n",
      "5007.pickle\n",
      "5008.pickle\n",
      "5009.pickle\n",
      "5010.pickle\n",
      "5011.pickle\n",
      "5012.pickle\n",
      "5013.pickle\n",
      "5014.pickle\n",
      "5015.pickle\n",
      "5016.pickle\n",
      "5017.pickle\n",
      "5018.pickle\n",
      "5019.pickle\n",
      "5020.pickle\n",
      "5021.pickle\n",
      "5022.pickle\n",
      "5023.pickle\n",
      "5024.pickle\n",
      "5025.pickle\n",
      "5026.pickle\n",
      "5027.pickle\n",
      "5028.pickle\n",
      "5029.pickle\n",
      "5030.pickle\n",
      "5031.pickle\n",
      "5032.pickle\n",
      "5033.pickle\n",
      "5034.pickle\n",
      "5035.pickle\n",
      "5036.pickle\n",
      "5037.pickle\n",
      "5038.pickle\n",
      "5039.pickle\n",
      "5040.pickle\n",
      "5041.pickle\n",
      "5042.pickle\n",
      "5043.pickle\n",
      "5044.pickle\n",
      "5045.pickle\n",
      "5046.pickle\n",
      "5047.pickle\n",
      "5048.pickle\n",
      "5049.pickle\n",
      "5050.pickle\n",
      "5051.pickle\n",
      "5052.pickle\n",
      "5053.pickle\n",
      "5054.pickle\n",
      "5055.pickle\n",
      "5056.pickle\n",
      "5057.pickle\n",
      "5058.pickle\n",
      "5059.pickle\n",
      "5060.pickle\n",
      "5061.pickle\n",
      "5062.pickle\n",
      "5063.pickle\n",
      "5064.pickle\n",
      "5065.pickle\n",
      "5066.pickle\n",
      "5067.pickle\n",
      "5068.pickle\n",
      "5069.pickle\n",
      "5070.pickle\n",
      "5071.pickle\n",
      "5072.pickle\n",
      "5073.pickle\n",
      "5074.pickle\n",
      "5075.pickle\n",
      "5076.pickle\n",
      "5077.pickle\n",
      "5078.pickle\n",
      "5079.pickle\n",
      "5080.pickle\n",
      "5081.pickle\n",
      "5082.pickle\n",
      "5083.pickle\n",
      "5084.pickle\n",
      "5085.pickle\n",
      "5086.pickle\n",
      "5087.pickle\n",
      "5088.pickle\n",
      "5089.pickle\n",
      "5090.pickle\n",
      "5091.pickle\n",
      "5092.pickle\n",
      "5093.pickle\n",
      "5094.pickle\n",
      "5095.pickle\n",
      "5096.pickle\n",
      "5097.pickle\n",
      "5098.pickle\n",
      "5099.pickle\n",
      "5100.pickle\n",
      "5101.pickle\n",
      "5102.pickle\n",
      "5103.pickle\n",
      "5104.pickle\n",
      "5105.pickle\n",
      "5106.pickle\n",
      "5107.pickle\n",
      "5108.pickle\n",
      "5109.pickle\n",
      "5110.pickle\n",
      "5111.pickle\n",
      "5112.pickle\n",
      "5113.pickle\n",
      "5114.pickle\n",
      "5115.pickle\n",
      "5116.pickle\n",
      "5117.pickle\n",
      "5118.pickle\n",
      "5119.pickle\n",
      "5120.pickle\n",
      "5121.pickle\n",
      "5122.pickle\n",
      "5123.pickle\n",
      "5124.pickle\n",
      "5125.pickle\n",
      "5126.pickle\n",
      "5127.pickle\n",
      "5128.pickle\n",
      "5129.pickle\n",
      "5130.pickle\n",
      "5131.pickle\n",
      "5132.pickle\n",
      "5133.pickle\n",
      "5134.pickle\n",
      "5135.pickle\n",
      "5136.pickle\n",
      "5137.pickle\n",
      "5138.pickle\n",
      "5139.pickle\n",
      "5140.pickle\n",
      "5141.pickle\n",
      "5142.pickle\n",
      "5143.pickle\n",
      "5144.pickle\n",
      "5145.pickle\n",
      "5146.pickle\n",
      "5147.pickle\n",
      "5148.pickle\n",
      "5149.pickle\n",
      "5150.pickle\n",
      "5151.pickle\n",
      "5152.pickle\n",
      "5153.pickle\n",
      "5154.pickle\n",
      "5155.pickle\n",
      "5156.pickle\n",
      "5157.pickle\n",
      "5158.pickle\n",
      "5159.pickle\n",
      "5160.pickle\n",
      "5161.pickle\n",
      "5162.pickle\n",
      "5163.pickle\n",
      "5164.pickle\n",
      "5165.pickle\n",
      "5166.pickle\n",
      "5167.pickle\n",
      "5168.pickle\n",
      "5169.pickle\n",
      "5170.pickle\n",
      "5171.pickle\n",
      "5172.pickle\n",
      "5173.pickle\n",
      "5174.pickle\n",
      "5175.pickle\n",
      "5176.pickle\n",
      "5177.pickle\n",
      "5178.pickle\n",
      "5179.pickle\n",
      "5180.pickle\n",
      "5181.pickle\n",
      "5182.pickle\n",
      "5183.pickle\n",
      "5184.pickle\n",
      "5185.pickle\n",
      "5186.pickle\n",
      "5187.pickle\n",
      "5188.pickle\n",
      "5189.pickle\n",
      "5190.pickle\n",
      "5191.pickle\n",
      "5192.pickle\n",
      "5193.pickle\n",
      "5194.pickle\n",
      "5195.pickle\n",
      "5196.pickle\n",
      "5197.pickle\n",
      "5198.pickle\n",
      "5199.pickle\n",
      "5200.pickle\n",
      "5201.pickle\n",
      "5202.pickle\n",
      "5203.pickle\n",
      "5204.pickle\n",
      "5205.pickle\n",
      "5206.pickle\n",
      "5207.pickle\n",
      "5208.pickle\n",
      "5209.pickle\n",
      "5210.pickle\n",
      "5211.pickle\n",
      "5212.pickle\n",
      "5213.pickle\n",
      "5214.pickle\n",
      "5215.pickle\n",
      "5216.pickle\n",
      "5217.pickle\n",
      "5218.pickle\n",
      "5219.pickle\n",
      "5220.pickle\n",
      "5221.pickle\n",
      "5222.pickle\n",
      "5223.pickle\n",
      "5224.pickle\n",
      "5225.pickle\n",
      "5226.pickle\n",
      "5227.pickle\n",
      "5228.pickle\n",
      "5229.pickle\n",
      "5230.pickle\n",
      "5231.pickle\n",
      "5232.pickle\n",
      "5233.pickle\n",
      "5234.pickle\n",
      "5235.pickle\n",
      "5236.pickle\n",
      "5237.pickle\n",
      "5238.pickle\n",
      "5239.pickle\n",
      "5240.pickle\n",
      "5241.pickle\n",
      "5242.pickle\n",
      "5243.pickle\n",
      "5244.pickle\n",
      "5245.pickle\n",
      "5246.pickle\n",
      "5247.pickle\n",
      "5248.pickle\n",
      "5249.pickle\n",
      "5250.pickle\n",
      "5251.pickle\n",
      "5252.pickle\n",
      "5253.pickle\n",
      "5254.pickle\n",
      "5255.pickle\n",
      "5256.pickle\n",
      "5257.pickle\n",
      "5258.pickle\n",
      "5259.pickle\n",
      "5260.pickle\n",
      "5261.pickle\n",
      "5262.pickle\n",
      "5263.pickle\n",
      "5264.pickle\n",
      "5265.pickle\n",
      "5266.pickle\n",
      "5267.pickle\n",
      "5268.pickle\n",
      "5269.pickle\n",
      "5270.pickle\n",
      "5271.pickle\n",
      "5272.pickle\n",
      "5273.pickle\n",
      "5274.pickle\n",
      "5275.pickle\n",
      "5276.pickle\n",
      "5277.pickle\n",
      "5278.pickle\n",
      "5279.pickle\n",
      "5280.pickle\n",
      "5281.pickle\n",
      "5282.pickle\n",
      "5283.pickle\n",
      "5284.pickle\n",
      "5285.pickle\n",
      "5286.pickle\n",
      "5287.pickle\n",
      "5288.pickle\n",
      "5289.pickle\n",
      "5290.pickle\n",
      "5291.pickle\n",
      "5292.pickle\n",
      "5293.pickle\n",
      "5294.pickle\n",
      "5295.pickle\n",
      "5296.pickle\n",
      "5297.pickle\n",
      "5298.pickle\n",
      "5299.pickle\n",
      "5300.pickle\n",
      "5301.pickle\n",
      "5302.pickle\n",
      "5303.pickle\n",
      "5304.pickle\n",
      "5305.pickle\n",
      "5306.pickle\n",
      "5307.pickle\n",
      "5308.pickle\n",
      "5309.pickle\n",
      "5310.pickle\n",
      "5311.pickle\n",
      "5312.pickle\n",
      "5313.pickle\n",
      "5314.pickle\n",
      "5315.pickle\n",
      "5316.pickle\n",
      "5317.pickle\n",
      "5318.pickle\n",
      "5319.pickle\n",
      "5320.pickle\n",
      "5321.pickle\n",
      "5322.pickle\n",
      "5323.pickle\n",
      "5324.pickle\n",
      "5325.pickle\n",
      "5326.pickle\n",
      "5327.pickle\n",
      "5328.pickle\n",
      "5329.pickle\n",
      "5330.pickle\n",
      "5331.pickle\n",
      "5332.pickle\n",
      "5333.pickle\n",
      "5334.pickle\n",
      "5335.pickle\n",
      "5336.pickle\n",
      "5337.pickle\n",
      "5338.pickle\n",
      "5339.pickle\n",
      "5340.pickle\n",
      "5341.pickle\n",
      "5342.pickle\n",
      "5343.pickle\n",
      "5344.pickle\n",
      "5345.pickle\n",
      "5346.pickle\n",
      "5347.pickle\n",
      "5348.pickle\n",
      "5349.pickle\n",
      "5350.pickle\n",
      "5351.pickle\n",
      "5352.pickle\n",
      "5353.pickle\n",
      "5354.pickle\n",
      "5355.pickle\n",
      "5356.pickle\n",
      "5357.pickle\n",
      "5358.pickle\n",
      "5359.pickle\n",
      "5360.pickle\n",
      "5361.pickle\n",
      "5362.pickle\n",
      "5363.pickle\n",
      "5364.pickle\n",
      "5365.pickle\n",
      "5366.pickle\n",
      "5367.pickle\n",
      "5368.pickle\n",
      "5369.pickle\n",
      "5370.pickle\n",
      "5371.pickle\n",
      "5372.pickle\n",
      "5373.pickle\n",
      "5374.pickle\n",
      "5375.pickle\n",
      "5376.pickle\n",
      "5377.pickle\n",
      "5378.pickle\n",
      "5379.pickle\n",
      "5380.pickle\n",
      "5381.pickle\n",
      "5382.pickle\n",
      "5383.pickle\n",
      "5384.pickle\n",
      "5385.pickle\n",
      "5386.pickle\n",
      "5387.pickle\n",
      "5388.pickle\n",
      "5389.pickle\n",
      "5390.pickle\n",
      "5391.pickle\n",
      "5392.pickle\n",
      "5393.pickle\n",
      "5394.pickle\n",
      "5395.pickle\n",
      "5396.pickle\n",
      "5397.pickle\n",
      "5398.pickle\n",
      "5399.pickle\n",
      "5400.pickle\n",
      "5401.pickle\n",
      "5402.pickle\n",
      "5403.pickle\n",
      "5404.pickle\n",
      "5405.pickle\n",
      "5406.pickle\n",
      "5407.pickle\n",
      "5408.pickle\n",
      "5409.pickle\n",
      "5410.pickle\n",
      "5411.pickle\n",
      "5412.pickle\n",
      "5413.pickle\n",
      "5414.pickle\n",
      "5415.pickle\n",
      "5416.pickle\n",
      "5417.pickle\n",
      "5418.pickle\n",
      "5419.pickle\n",
      "5420.pickle\n",
      "5421.pickle\n",
      "5422.pickle\n",
      "5423.pickle\n",
      "5424.pickle\n",
      "5425.pickle\n",
      "5426.pickle\n",
      "5427.pickle\n",
      "5428.pickle\n",
      "5429.pickle\n",
      "5430.pickle\n",
      "5431.pickle\n",
      "5432.pickle\n",
      "5433.pickle\n",
      "5434.pickle\n",
      "5435.pickle\n",
      "5436.pickle\n",
      "5437.pickle\n",
      "5438.pickle\n",
      "5439.pickle\n",
      "5440.pickle\n",
      "5441.pickle\n",
      "5442.pickle\n",
      "5443.pickle\n",
      "5444.pickle\n",
      "5445.pickle\n",
      "5446.pickle\n",
      "5447.pickle\n",
      "5448.pickle\n",
      "5449.pickle\n",
      "5450.pickle\n",
      "5451.pickle\n",
      "5452.pickle\n",
      "5453.pickle\n",
      "5454.pickle\n",
      "5455.pickle\n",
      "5456.pickle\n",
      "5457.pickle\n",
      "5458.pickle\n",
      "5459.pickle\n",
      "5460.pickle\n",
      "5461.pickle\n",
      "5462.pickle\n",
      "5463.pickle\n",
      "5464.pickle\n",
      "5465.pickle\n",
      "5466.pickle\n",
      "5467.pickle\n",
      "5468.pickle\n",
      "5469.pickle\n",
      "5470.pickle\n",
      "5471.pickle\n",
      "5472.pickle\n",
      "5473.pickle\n",
      "5474.pickle\n",
      "5475.pickle\n",
      "5476.pickle\n",
      "5477.pickle\n",
      "5478.pickle\n",
      "5479.pickle\n",
      "5480.pickle\n",
      "5481.pickle\n",
      "5482.pickle\n",
      "5483.pickle\n",
      "5484.pickle\n",
      "5485.pickle\n",
      "5486.pickle\n",
      "5487.pickle\n",
      "5488.pickle\n",
      "5489.pickle\n",
      "5490.pickle\n",
      "5491.pickle\n",
      "5492.pickle\n",
      "5493.pickle\n",
      "5494.pickle\n",
      "5495.pickle\n",
      "5496.pickle\n",
      "5497.pickle\n",
      "5498.pickle\n",
      "5499.pickle\n",
      "5500.pickle\n",
      "5501.pickle\n",
      "5502.pickle\n",
      "5503.pickle\n",
      "5504.pickle\n",
      "5505.pickle\n",
      "5506.pickle\n",
      "5507.pickle\n",
      "5508.pickle\n",
      "5509.pickle\n",
      "5510.pickle\n",
      "5511.pickle\n",
      "5512.pickle\n",
      "5513.pickle\n",
      "5514.pickle\n",
      "5515.pickle\n",
      "5516.pickle\n",
      "5517.pickle\n",
      "5518.pickle\n",
      "5519.pickle\n",
      "5520.pickle\n",
      "5521.pickle\n",
      "5522.pickle\n",
      "5523.pickle\n",
      "5524.pickle\n",
      "5525.pickle\n",
      "5526.pickle\n",
      "5527.pickle\n",
      "5528.pickle\n",
      "5529.pickle\n",
      "5530.pickle\n",
      "5531.pickle\n",
      "5532.pickle\n",
      "5533.pickle\n",
      "5534.pickle\n",
      "5535.pickle\n",
      "5536.pickle\n",
      "5537.pickle\n",
      "5538.pickle\n",
      "5539.pickle\n",
      "5540.pickle\n",
      "5541.pickle\n",
      "5542.pickle\n",
      "5543.pickle\n",
      "5544.pickle\n",
      "5545.pickle\n",
      "5546.pickle\n",
      "5547.pickle\n",
      "5548.pickle\n",
      "5549.pickle\n",
      "5550.pickle\n",
      "5551.pickle\n",
      "5552.pickle\n",
      "5553.pickle\n",
      "5554.pickle\n",
      "5555.pickle\n",
      "5556.pickle\n"
     ]
    }
   ],
   "source": [
    "def inference(model, data_test, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in DataLoader(data_test, batch_size=1):\n",
    "            data = data.to(device)\n",
    "            pred = torch.argmax(model(data), dim=-1)\n",
    "\n",
    "            pred = pred.cpu().numpy()\n",
    "\n",
    "            file_name = data.file_name\n",
    "\n",
    "            assert len(file_name) == 1\n",
    "\n",
    "            yield file_name[0], pred\n",
    "\n",
    "inv_room_mapping = {val: key for key, val in constants.ROOM_MAPPING.items()}\n",
    "\n",
    "for file_name, pred in inference(model, ds_test, device=\"cpu\"):\n",
    "    graph_nx = load_pickle(os.path.join(ds_test.graph_path, file_name))\n",
    "\n",
    "    for node in graph_nx.nodes:\n",
    "        graph_nx.nodes[node]['room_type'] = pred[node]\n",
    "\n",
    "    # print(graph_nx.nodes(data=True))\n",
    "\n",
    "    print(file_name)\n",
    "\n",
    "    for node in graph_nx.nodes:\n",
    "        room_name = constants.ROOM_NAMES[graph_nx.nodes[node]['room_type']]\n",
    "\n",
    "        room_name = inv_room_mapping[room_name]\n",
    "\n",
    "        pred_zone_name = constants.ZONING_MAPPING[room_name]\n",
    "\n",
    "        zone_name = constants.ZONING_NAMES[graph_nx.nodes[node]['zoning_type']]\n",
    "\n",
    "        # Check that the predictions make sense according to the room type -> zoning type mapping\n",
    "        assert zone_name == pred_zone_name, f\"Zone name: {zone_name} Pred zone name: {pred_zone_name}\"\n",
    "    \n",
    "    save_pickle(graph_nx, os.path.join(pred_graph_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATModel(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(4, 32, heads=1)\n",
       "    (1): GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (linear): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
