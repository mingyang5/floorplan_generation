{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\") # your_huggingface_token\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,3,5,6,7'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL 2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InternVL 2.5 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a26873c9974e0a936cc51551d94d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: row_039.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (3, 2, 1)\"\n",
      "\n",
      "Image: row_003.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: The analysis based on visual similarity indicates the following ordering from closest match to least similar:\n",
      "\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Therefore, the most similar generated sub-image is #2, followed by #1, and the least similar is #3.\n",
      "\n",
      "Image: row_247.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 1, Similarity Order: (2, 3, 0)\n",
      "\n",
      "Image: row_006.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (1, 2, 3)\n",
      "\n",
      "Image: row_146.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (2, 1, 3)\n",
      "\n",
      "Image: row_152.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (1, 2, 3)\n",
      "\n",
      "Image: row_028.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 1, 2)\n",
      "\n",
      "Image: row_001.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (2, 3, 1)\n",
      "\n",
      "Image: row_192.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 2, 1)\n",
      "\n",
      "Image: row_007.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 1, Similarity Order: (3, 2, 0)\"\n",
      "\n",
      "Image: row_000.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 1, 2)\n",
      "\n",
      "Image: row_158.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 1, 2)\n",
      "\n",
      "Image: row_055.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (2, 1, 3)\n",
      "\n",
      "Image: row_166.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (3, 1, 2)\"\n",
      "\n",
      "Image: row_101.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (3, 1, 2)\"\n",
      "\n",
      "Image: row_276.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 2, 1)\n",
      "\n",
      "Image: row_184.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 2, 1)\n",
      "\n",
      "Image: row_208.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (3, 2, 1)\"\n",
      "\n",
      "Image: row_271.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: After analyzing the provided images, it looks like the generated variation closest to the ground truth is image #1. Image #2 seems the least similar after image #1. \n",
      "\n",
      "So, my response is:\n",
      "\"Ground Truth: 0, Similarity Order: (1, 3, 2)\"\n",
      "\n",
      "Image: row_004.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (1, 3, 2)\"\n",
      "\n",
      "Image: row_008.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 1\n",
      "Similarity Order: (0, 3, 2)\n",
      "\n",
      "Image: row_002.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 1, Similarity Order: (3, 2, 0)\n",
      "\n",
      "Image: row_328.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (3, 1, 2)\n",
      "\n",
      "Image: row_239.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 1, Similarity Order: (3, 2, 0)\n",
      "\n",
      "Image: row_012.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Image: row_319.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: Ground Truth: 0, Similarity Order: (2, 1, 3)\n",
      "\n",
      "Image: row_005.png\n",
      "User: <image>\n",
      "You will be given an image composed of four sub-images arranged in a row. The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\n",
      "\n",
      "Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). Then order these three generated images by similarity, from the closest match to the least similar.\n",
      "\n",
      "Please provide your answer in the following format:\n",
      "\"Ground Truth: 1, Similarity Order: (X, Y, Z)\"\n",
      "where X, Y, and Z are the image numbers of the generated images in order of similarity.\n",
      "\n",
      "For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\n",
      "\"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Now, analyze the provided image and give your response.\n",
      "Assistant: \"Ground Truth: 0, Similarity Order: (2, 1, 3)\"\n",
      "\n",
      "Results saved to results_mingyang.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "path = 'OpenGVLab/InternVL2_5-8B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "#pixel_values = load_image('/home/mingyang/workspace/layout/outputs.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "#generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\n",
    "#question = '<image>\\nPlease describe the image shortly.'\n",
    "#response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "#print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "#pixel_values = load_image('./house-diffusion/outputs/raws_new/row_002.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\n",
    "question = (\n",
    "    \"<image>\\n\"\n",
    "    \"You will be given an image composed of four sub-images arranged in a row. \"\n",
    "    \"The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. \"\n",
    "    \"The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\\n\\n\"\n",
    "    \"Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). \"\n",
    "    \"Then order these three generated images by similarity, from the closest match to the least similar.\\n\\n\"\n",
    "    \"Please provide your answer in the following format:\\n\"\n",
    "    \"\\\"Ground Truth: 1, Similarity Order: (X, Y, Z)\\\"\\n\"\n",
    "    \"where X, Y, and Z are the image numbers of the generated images in order of similarity.\\n\\n\"\n",
    "    \"For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\\n\"\n",
    "    \"\\\"Ground Truth: 0, Similarity Order: (2, 1, 3)\\\"\\n\\n\"\n",
    "    \"Now, analyze the provided image and give your response.\"\n",
    ")\n",
    "#response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "#print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "image_directory = \"./house-diffusion/outputs/user_study_mingyang/user_study\"\n",
    "\n",
    "# Get all image file paths. Adjust the file extension filter if needed.\n",
    "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "results = {}\n",
    "\n",
    "# Regex to capture the similarity order numbers from the response.\n",
    "pattern = re.compile(r'Similarity Order:\\s*\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)')\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(image_directory, img_file)\n",
    "    pixel_values = load_image(img_path, max_num=12).to(torch.bfloat16).cuda()\n",
    "    \n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    print(f\"Image: {img_file}\")\n",
    "    print(f\"User: {question}\\nAssistant: {response}\\n\")\n",
    "    match = pattern.search(response)\n",
    "    if match:\n",
    "        order = [int(match.group(1)), int(match.group(2)), int(match.group(3))]\n",
    "        results[img_file] = order\n",
    "    else:\n",
    "        # If the response doesn't match the expected format, you can handle it here.\n",
    "        results[img_file] = None\n",
    "output_path = \"results_mingyang.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InternVL 2.5 26B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingyang/.conda/envs/video/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ee5562cf2c461ab2258ac09c2c3924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,3,5,6,7'\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,\n",
    "        'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "path = \"OpenGVLab/InternVL2_5-26B\"\n",
    "device_map = split_model('InternVL2_5-26B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = load_image('./house-diffusion/outputs/raws_/row_000.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m question \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will be given an image composed of four sub-images arranged in a row. The first sub-image (the leftmost one) is the ground truth floor plan for a house. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow, analyze the provided image and give your response.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m'''question = 'Please write a poem according to the image.'\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mprint(f'User: {question}\\nAssistant: {response}')'''\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/OpenGVLab/InternVL2_5-26B/a5d2234c494a87fee81ac05a45e79ef9c8a6b505/modeling_internvl_chat.py:291\u001b[0m, in \u001b[0;36mInternVLChatModel.chat\u001b[0;34m(self, tokenizer, pixel_values, question, generation_config, history, return_history, num_patches_list, IMG_START_TOKEN, IMG_END_TOKEN, IMG_CONTEXT_TOKEN, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    290\u001b[0m generation_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[0;32m--> 291\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generation_output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    298\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(template\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mstrip())[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.conda/envs/video/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/OpenGVLab/InternVL2_5-26B/a5d2234c494a87fee81ac05a45e79ef9c8a6b505/modeling_internvl_chat.py:340\u001b[0m, in \u001b[0;36mInternVLChatModel.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, visual_features, generation_config, output_hidden_states, **generate_kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[0;32m--> 340\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/video/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/video/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/video/lib/python3.10/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/video/lib/python3.10/site-packages/transformers/generation/utils.py:2413\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pixel_values = load_image('./house-diffusion/outputs/raws_/row_000.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\n",
    "question = (\n",
    "    \"<image>\\n\"\n",
    "    \"You will be given an image composed of four sub-images arranged in a row. The first sub-image (the leftmost one) is the ground truth floor plan for a house. \"\n",
    "    \"The next three sub-images (to the right of the ground truth image) are generated variations from different diffusion models. \"\n",
    "    \"Your task is to determine which of these three generated sub-images is most similar to the ground truth and then order all three generated images by similarity, \"\n",
    "    \"from the closest match to the least similar.\\n\\n\"\n",
    "    \"Please provide your answer in the following format:\\n\"\n",
    "    \"\\\"Ground Truth: [ID of the ground truth image], Similarity Order: ([ID of closest image], [ID of second closest image], [ID of third closest image])\\\"\\n\\n\"\n",
    "    \"For example, if the ground truth image is #55 and the order of similarity you determine is the second image first, then the first image, then the third image, you would answer:\\n\"\n",
    "    \"\\\"Ground Truth: 55, Similarity Order: (2, 1, 3)\\\"\\n\\n\"\n",
    "    \"Now, analyze the provided image and give your response.\"\n",
    ")\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "'''question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
