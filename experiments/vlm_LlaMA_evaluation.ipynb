{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\") # your_huggingface_token\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6,7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlaMA-3.2-11B-Vision-Instruct\n",
    "\n",
    "The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b6c56890864eb4a8e4215f6e202a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "local_image_path = \"./house-diffusion/outputs/use_study_xiyuan/use_study\" \n",
    "#image = Image.open(local_image_path)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"alt\": \"A horizontally arranged image with four sub-images: the first one is the ground truth floor plan, followed by three generated variations.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You will be given an image composed of four sub-images arranged in a row. \"\n",
    "                    \"The first sub-image (image #0, the leftmost) is the ground truth floor plan for a house. \"\n",
    "                    \"The next three sub-images (images #1, #2, and #3, from left to right) are generated variations from different diffusion models.\\n\\n\"\n",
    "                    \"Your task: Determine which of these three generated sub-images (#1, #2, #3) is most similar to the ground truth (#0). \"\n",
    "                    \"Then order these three generated images by similarity, from the closest match to the least similar.\\n\\n\"\n",
    "                    \"Please provide your answer in the following format:\\n\"\n",
    "                    \"\\\"Ground Truth: 0, Similarity Order: (X, Y, Z)\\\"\\n\"\n",
    "                    \"where X, Y, and Z are the image numbers of the generated images in order of similarity.\\n\\n\"\n",
    "                    \"For example, if the most similar image is #2, the second most similar is #1, and the least similar is #3, you would answer:\\n\"\n",
    "                    \"\\\"Ground Truth: 1, Similarity Order: (2, 1, 3)\\\"\\n\\n\"\n",
    "                    \"Now, analyze the provided image and give your response.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "pattern = re.compile(r\"Similarity Order:\\s*\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)\")\n",
    "explanation_pattern = re.compile(r'Explanation:\\s*(.*)', re.DOTALL)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for filename in os.listdir(local_image_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(local_image_path, filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=500)  # Increased tokens for safer output\n",
    "        decoded_response = processor.decode(output[0])\n",
    "\n",
    "        # Extract order\n",
    "        order_match = pattern.search(decoded_response)\n",
    "        # Extract explanation\n",
    "        explanation_match = explanation_pattern.search(decoded_response)\n",
    "\n",
    "        if order_match:\n",
    "            X, Y, Z = order_match.groups()\n",
    "            order = [int(X), int(Y), int(Z)]\n",
    "        else:\n",
    "            order = None\n",
    "\n",
    "        if explanation_match:\n",
    "            reason = explanation_match.group(1).strip()\n",
    "        else:\n",
    "            reason = None\n",
    "\n",
    "        base_id = os.path.splitext(filename)[0]\n",
    "        results[base_id] = {\n",
    "            \"order\": order,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "\n",
    "# Save results to JSON\n",
    "output_json_path = \"results.json\"\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
